#include "ai/cross_modal_guidance.hpp"
#include "tensor.hpp"
#include <iostream>
#include <iomanip>
#include <vector>
#include <chrono>

using namespace clmodel;
using namespace clmodel::ai;

/**
 * @brief Comprehensive demo for Week 14: Cross-Modal Guidance Systems
 * 
 * This demo showcases:
 * 1. Cross-modal attention mechanisms
 * 2. Advanced guidance conditioning
 * 3. Dynamic guidance strength adaptation
 * 4. Inter-modal communication bridges
 * 5. Adaptive pipeline management
 * 6. Multi-modal coordination
 * 7. Real-time guidance effectiveness analysis
 */

void printSectionHeader(const std::string& title) {
    std::cout << "\n" << std::string(80, '=') << std::endl;
    std::cout << "  " << title << std::endl;
    std::cout << std::string(80, '=') << std::endl;
}

void printSubsectionHeader(const std::string& title) {
    std::cout << "\n" << std::string(60, '-') << std::endl;
    std::cout << "  " << title << std::endl;
    std::cout << std::string(60, '-') << std::endl;
}

void printGuidanceMetrics(const GuidanceMetrics& metrics, const std::string& prefix = "") {
    std::cout << std::fixed << std::setprecision(3);
    std::cout << prefix << "Guidance Metrics:" << std::endl;
    std::cout << prefix << "  Effectiveness Score: " << metrics.effectiveness_score << std::endl;
    std::cout << prefix << "  Alignment Quality:   " << metrics.alignment_quality << std::endl;
    std::cout << prefix << "  Semantic Consistency:" << metrics.semantic_consistency << std::endl;
    std::cout << prefix << "  Temporal Coherence:  " << metrics.temporal_coherence << std::endl;
    std::cout << prefix << "  Computational Cost:  " << metrics.computational_cost << " ms" << std::endl;
    std::cout << prefix << "  Guidance Mode:       " << metrics.guidance_mode << std::endl;
}

Tensor createMockModalityData(const std::string& modality_type, size_t batch_size = 4, size_t feature_dim = 512) {
    std::cout << "  Creating mock " << modality_type << " data [" << batch_size << "x" << feature_dim << "]" << std::endl;
    
    // Create realistic mock data for different modalities
    Tensor data = Tensor::randn({batch_size, feature_dim}, 0.0, 1.0);
    
    if (modality_type == "visual") {
        // Visual features might have spatial structure
        data = data * 0.8 + 0.1; // Slightly constrained range
    } else if (modality_type == "audio") {
        // Audio features might be more sparse
        data = data * 0.6;    } else if (modality_type == "text") {
        // Text features might be more discrete-like
        data = data * 0.5; // Constrain range instead of clamping
    }
    
    return data;
}

void demonstrateCrossModalAttention() {
    printSectionHeader("1. Cross-Modal Attention Mechanisms");
    
    // Create mock multi-modal data
    printSubsectionHeader("Creating Multi-Modal Data");
    Tensor visual_features = createMockModalityData("visual", 4, 512);
    Tensor audio_features = createMockModalityData("audio", 4, 512);
    Tensor text_features = createMockModalityData("text", 4, 512);
      // Initialize cross-modal attention
    printSubsectionHeader("Initializing Advanced Cross-Modal Attention");
    AdvancedCrossModalAttention attention(512, 512, 512, 8);
    attention.setGuidanceStrength(0.7);
    attention.setTemperature(0.8);
    
    std::cout << "  Guidance Strength: " << attention.getGuidanceStrength() << std::endl;
    
    // Demonstrate different attention configurations
    printSubsectionHeader("Audio-Visual Cross-Modal Attention");
    Tensor av_attended = attention.forward(audio_features, visual_features, visual_features);
    GuidanceMetrics av_metrics = attention.computeGuidanceMetrics(audio_features, av_attended);
    printGuidanceMetrics(av_metrics, "  ");
    
    printSubsectionHeader("Text-Visual Cross-Modal Attention");
    Tensor tv_attended = attention.forward(text_features, visual_features, visual_features);
    GuidanceMetrics tv_metrics = attention.computeGuidanceMetrics(text_features, tv_attended);
    printGuidanceMetrics(tv_metrics, "  ");
    
    printSubsectionHeader("Text-Audio Cross-Modal Attention");
    Tensor ta_attended = attention.forward(text_features, audio_features, audio_features);
    GuidanceMetrics ta_metrics = attention.computeGuidanceMetrics(text_features, ta_attended);
    printGuidanceMetrics(ta_metrics, "  ");
    
    std::cout << "âœ“ Cross-Modal Attention demonstration completed" << std::endl;
}

void demonstrateCrossModalConditioner() {
    printSectionHeader("2. Advanced Cross-Modal Conditioning");
    
    // Test different guidance types
    std::vector<GuidanceType> guidance_types = {
        GuidanceType::ATTENTION_BASED,
        GuidanceType::FEATURE_ALIGNMENT,
        GuidanceType::SEMANTIC_BRIDGE,
        GuidanceType::TEMPORAL_SYNC,
        GuidanceType::ADAPTIVE_WEIGHTED
    };
    
    std::vector<std::string> guidance_names = {
        "Attention-Based", "Feature Alignment", "Semantic Bridge", 
        "Temporal Sync", "Adaptive Weighted"
    };
    
    // Create test modalities
    Tensor visual_data = createMockModalityData("visual", 6, 256);
    Tensor audio_data = createMockModalityData("audio", 6, 256);
    
    for (size_t i = 0; i < guidance_types.size(); ++i) {
        printSubsectionHeader("Testing " + guidance_names[i] + " Guidance");
        
        GuidanceConfig config;
        config.guidance_type = guidance_types[i];
        config.guidance_strength = 0.6;
        config.adaptation_rate = 0.15;
        config.enable_temporal_sync = true;
        config.adaptive_mode = true;
        
        CrossModalConditioner conditioner(config);
        
        // Apply conditioning
        Tensor conditioned_visual = conditioner.conditionModality(
            visual_data, audio_data, "visual", "audio");
        
        GuidanceMetrics metrics = conditioner.getLastGuidanceMetrics();
        printGuidanceMetrics(metrics, "  ");
        
        // Test modality affinities
        auto affinities = conditioner.getModalityAffinities();
        if (!affinities.empty()) {
            std::cout << "  Modality Affinities:" << std::endl;
            for (const auto& pair : affinities) {
                std::cout << "    " << pair.first << ": " << std::fixed 
                         << std::setprecision(3) << pair.second << std::endl;
            }
        }
    }
    
    // Test multi-modal conditioning
    printSubsectionHeader("Multi-Modal Conditioning");
    std::map<std::string, Tensor> modalities = {
        {"visual", createMockModalityData("visual", 4, 256)},
        {"audio", createMockModalityData("audio", 4, 256)},
        {"text", createMockModalityData("text", 4, 256)}
    };
    
    CrossModalConditioner multi_conditioner;
    auto conditioned_modalities = multi_conditioner.conditionMultiModal(modalities);
    
    std::cout << "  Conditioned " << conditioned_modalities.size() << " modalities" << std::endl;
    for (const auto& pair : conditioned_modalities) {
        std::cout << "    " << pair.first << ": [" << pair.second.shape()[0] 
                 << "x" << pair.second.shape()[1] << "]" << std::endl;
    }
    
    std::cout << "âœ“ Cross-Modal Conditioning demonstration completed" << std::endl;
}

void demonstrateGuidanceController() {
    printSectionHeader("3. Dynamic Guidance Strength Adaptation");
    
    printSubsectionHeader("Initializing Guidance Controller");
    GuidanceController controller(0.5, 0.1);
    controller.enableAdaptiveControl(true);
    controller.setQualityThreshold(0.75);
    
    std::cout << "  Initial Strength: " << controller.getCurrentStrength() << std::endl;
    std::cout << "  Adaptive Control: Enabled" << std::endl;
    std::cout << "  Quality Threshold: 0.75" << std::endl;
    
    // Simulate guidance feedback over time
    printSubsectionHeader("Simulating Adaptive Guidance Evolution");
    std::vector<double> effectiveness_sequence = {
        0.4, 0.55, 0.62, 0.71, 0.68, 0.78, 0.82, 0.79, 0.85, 0.88
    };
    
    std::cout << "  Step | Effectiveness | Strength | Stabilized?" << std::endl;
    std::cout << "  -----|---------------|----------|-------------" << std::endl;
    
    for (size_t step = 0; step < effectiveness_sequence.size(); ++step) {
        // Create mock guidance metrics
        GuidanceMetrics feedback;
        feedback.effectiveness_score = effectiveness_sequence[step];
        feedback.alignment_quality = effectiveness_sequence[step] * 0.9;
        feedback.semantic_consistency = effectiveness_sequence[step] * 0.85;
        feedback.temporal_coherence = 0.8;
        feedback.computational_cost = 50.0 + step * 5.0;
        
        controller.updateStrength(feedback);
        
        std::cout << std::fixed << std::setprecision(3);
        std::cout << "  " << std::setw(4) << step + 1 
                 << " | " << std::setw(13) << feedback.effectiveness_score
                 << " | " << std::setw(8) << controller.getCurrentStrength()
                 << " | " << (controller.isStabilized() ? "Yes" : "No") << std::endl;
    }
    
    // Analyze controller history
    printSubsectionHeader("Controller Analysis");
    auto strength_history = controller.getStrengthHistory();
    double avg_effectiveness = controller.getAverageEffectiveness();
    
    std::cout << "  History Length: " << strength_history.size() << std::endl;
    std::cout << "  Average Effectiveness: " << std::fixed << std::setprecision(3) 
             << avg_effectiveness << std::endl;
    std::cout << "  Final Stabilized: " << (controller.isStabilized() ? "Yes" : "No") << std::endl;
    
    std::cout << "âœ“ Guidance Controller demonstration completed" << std::endl;
}

void demonstrateModalBridgeNetwork() {
    printSectionHeader("4. Inter-Modal Communication Bridges");
    
    // Initialize bridge network
    printSubsectionHeader("Initializing Modal Bridge Network");
    std::vector<std::string> modalities = {"visual", "audio", "text", "haptic"};
    ModalBridgeNetwork bridge_network(modalities);
    
    std::cout << "  Supported Modalities: ";
    for (const auto& mod : modalities) {
        std::cout << mod << " ";
    }
    std::cout << std::endl;
    
    // Create bridges between modalities
    printSubsectionHeader("Creating Inter-Modal Bridges");
    bridge_network.createBridge("visual", "audio");
    bridge_network.createBridge("audio", "text");
    bridge_network.createBridge("text", "haptic");
    bridge_network.createBridge("visual", "text");
    bridge_network.createBridge("audio", "haptic");
    
    // Test bridge connectivity
    for (const auto& modality : modalities) {
        auto connected = bridge_network.getConnectedModalities(modality);
        std::cout << "  " << modality << " connected to: ";
        for (const auto& conn : connected) {
            std::cout << conn << " ";
        }
        std::cout << std::endl;
    }
    
    // Test feature bridging
    printSubsectionHeader("Feature Bridging and Semantic Alignment");
    Tensor visual_features = createMockModalityData("visual", 4, 128);
    
    // Bridge visual features to other modalities
    std::vector<std::string> target_modalities = {"audio", "text", "haptic"};
    for (const auto& target : target_modalities) {
        Tensor bridged_features = bridge_network.bridgeFeatures(
            visual_features, "visual", target);
        std::cout << "  Visual -> " << target << ": [" 
                 << bridged_features.shape()[0] << "x" << bridged_features.shape()[1] << "]" << std::endl;
    }
    
    // Test semantic distance computation
    printSubsectionHeader("Semantic Distance Analysis");
    Tensor audio_features = createMockModalityData("audio", 4, 128);
    Tensor text_features = createMockModalityData("text", 4, 128);
    
    double vis_aud_distance = bridge_network.computeSemanticDistance(visual_features, audio_features);
    double vis_text_distance = bridge_network.computeSemanticDistance(visual_features, text_features);
    double aud_text_distance = bridge_network.computeSemanticDistance(audio_features, text_features);
    
    std::cout << std::fixed << std::setprecision(3);
    std::cout << "  Visual-Audio Distance:  " << vis_aud_distance << std::endl;
    std::cout << "  Visual-Text Distance:   " << vis_text_distance << std::endl;
    std::cout << "  Audio-Text Distance:    " << aud_text_distance << std::endl;
    
    // Test network optimization
    printSubsectionHeader("Bridge Network Optimization");
    bridge_network.optimizeBridgeNetwork();
    auto bridge_strengths = bridge_network.getBridgeStrengths();
    
    std::cout << "  Optimized Bridge Strengths:" << std::endl;
    for (const auto& bridge : bridge_strengths) {
        std::cout << "    " << bridge.first.first << " -> " << bridge.first.second 
                 << ": " << std::fixed << std::setprecision(3) << bridge.second << std::endl;
    }
    
    std::cout << "âœ“ Modal Bridge Network demonstration completed" << std::endl;
}

void demonstrateMultiModalCoordinator() {
    printSectionHeader("5. Multi-Modal System Coordination");
    
    // Initialize coordinator
    printSubsectionHeader("Initializing Multi-Modal Coordinator");
    MultiModalCoordinator coordinator;
    
    std::vector<std::string> modalities = {"visual", "audio", "text"};
    bool init_success = coordinator.initialize(modalities);
    std::cout << "  Initialization: " << (init_success ? "Success" : "Failed") << std::endl;
    
    if (!init_success) {
        std::cout << "  âœ— Coordinator initialization failed" << std::endl;
        return;
    }
    
    // Test component access
    printSubsectionHeader("Testing Component Integration");
    auto& conditioner = coordinator.getConditioner();
    auto& guidance_controller = coordinator.getGuidanceController();
    auto& bridge_network = coordinator.getBridgeNetwork();
    
    std::cout << "  Conditioner: Accessible" << std::endl;
    std::cout << "  Guidance Controller: Accessible" << std::endl;
    std::cout << "  Bridge Network: Accessible" << std::endl;
    
    // Configure global guidance
    printSubsectionHeader("Global Guidance Configuration");
    GuidanceConfig global_config;
    global_config.guidance_type = GuidanceType::ADAPTIVE_WEIGHTED;
    global_config.guidance_strength = 0.7;
    global_config.adaptation_rate = 0.12;
    global_config.enable_temporal_sync = true;
    global_config.adaptive_mode = true;
    global_config.modality_weights = {
        {"visual", 0.4},
        {"audio", 0.35},
        {"text", 0.25}
    };
    
    coordinator.setGlobalGuidanceConfig(global_config);
    std::cout << "  Global guidance configuration applied" << std::endl;
    std::cout << "  Guidance Type: Adaptive Weighted" << std::endl;
    std::cout << "  Guidance Strength: " << global_config.guidance_strength << std::endl;
    
    // Test coordinated guidance
    printSubsectionHeader("Coordinated Multi-Modal Guidance");
    std::map<std::string, Tensor> test_modalities = {
        {"visual", createMockModalityData("visual", 6, 256)},
        {"audio", createMockModalityData("audio", 6, 256)},
        {"text", createMockModalityData("text", 6, 256)}
    };
    
    bool guidance_success = coordinator.coordinateGuidance(test_modalities, global_config);
    std::cout << "  Guidance Coordination: " << (guidance_success ? "Success" : "Failed") << std::endl;
    
    // Get system metrics
    auto system_metrics = coordinator.getSystemGuidanceMetrics();
    if (!system_metrics.empty()) {
        std::cout << "  System Guidance Metrics:" << std::endl;
        for (const auto& pair : system_metrics) {
            std::cout << "    " << pair.first << ":" << std::endl;
            printGuidanceMetrics(pair.second, "      ");
        }
    }
    
    // Test system optimization
    printSubsectionHeader("System Optimization");
    coordinator.optimizeGuidanceSystem();
    std::cout << "  System optimization completed" << std::endl;
    
    // Clean shutdown
    coordinator.shutdown();
    std::cout << "  Coordinator shutdown completed" << std::endl;
    
    std::cout << "âœ“ Multi-Modal Coordinator demonstration completed" << std::endl;
}

void demonstrateCrossModalUtils() {
    printSectionHeader("6. Cross-Modal Utility Functions");
    
    // Create test tensors
    printSubsectionHeader("Creating Test Data");
    Tensor tensor1 = createMockModalityData("test1", 4, 64);
    Tensor tensor2 = createMockModalityData("test2", 4, 64);
    Tensor tensor3 = tensor1 * 0.8 + 0.2; // Similar but not identical
    
    // Test similarity metrics
    printSubsectionHeader("Similarity and Distance Metrics");
    double cosine_sim_12 = CrossModalUtils::computeCosineSimilarity(tensor1, tensor2);
    double cosine_sim_13 = CrossModalUtils::computeCosineSimilarity(tensor1, tensor3);
    double euclidean_dist_12 = CrossModalUtils::computeEuclideanDistance(tensor1, tensor2);
    double euclidean_dist_13 = CrossModalUtils::computeEuclideanDistance(tensor1, tensor3);
    
    std::cout << std::fixed << std::setprecision(4);
    std::cout << "  Cosine Similarity (T1-T2):    " << cosine_sim_12 << std::endl;
    std::cout << "  Cosine Similarity (T1-T3):    " << cosine_sim_13 << std::endl;
    std::cout << "  Euclidean Distance (T1-T2):   " << euclidean_dist_12 << std::endl;
    std::cout << "  Euclidean Distance (T1-T3):   " << euclidean_dist_13 << std::endl;
    
    // Test feature space analysis
    printSubsectionHeader("Feature Space Analysis");
    size_t num_components = 16;
    Tensor pca_features = CrossModalUtils::computePrincipalComponents(tensor1, num_components);
    std::cout << "  PCA Components: [" << pca_features.shape()[0] 
             << "x" << pca_features.shape()[1] << "]" << std::endl;
    
    // Create projection matrix for common space
    Tensor projection_matrix = Tensor::randn({64, 32}, 0.0, 0.1);
    Tensor projected_features = CrossModalUtils::projectToCommonSpace(tensor1, projection_matrix);
    std::cout << "  Projected Features: [" << projected_features.shape()[0] 
             << "x" << projected_features.shape()[1] << "]" << std::endl;
    
    // Test guidance effectiveness evaluation
    printSubsectionHeader("Guidance Effectiveness Evaluation");
    GuidanceMetrics eval_metrics = CrossModalUtils::evaluateGuidanceEffectiveness(tensor3, tensor1);
    printGuidanceMetrics(eval_metrics, "  ");
    
    // Test guidance stability analysis
    printSubsectionHeader("Guidance Stability Analysis");
    std::vector<GuidanceMetrics> metrics_history;
    for (int i = 0; i < 10; ++i) {
        GuidanceMetrics metrics;
        metrics.effectiveness_score = 0.6 + 0.03 * i + (std::rand() % 100) / 1000.0;
        metrics.alignment_quality = metrics.effectiveness_score * 0.9;
        metrics.semantic_consistency = metrics.effectiveness_score * 0.85;
        metrics.temporal_coherence = 0.8;
        metrics_history.push_back(metrics);
    }
    
    double stability = CrossModalUtils::computeGuidanceStability(metrics_history);
    std::cout << "  Guidance Stability Score: " << std::fixed << std::setprecision(4) 
             << stability << std::endl;
    
    std::cout << "âœ“ Cross-Modal Utilities demonstration completed" << std::endl;
}

void runPerformanceBenchmark() {
    printSectionHeader("7. Performance Benchmark");
    
    std::cout << "Running performance tests for Week 14 Cross-Modal Guidance..." << std::endl;
      // Benchmark attention computation
    printSubsectionHeader("Advanced Cross-Modal Attention Performance");
    auto start = std::chrono::high_resolution_clock::now();
    
    AdvancedCrossModalAttention attention(512, 512, 512, 8);
    Tensor query = createMockModalityData("query", 16, 512);
    Tensor key = createMockModalityData("key", 16, 512);
    Tensor value = createMockModalityData("value", 16, 512);
    
    for (int i = 0; i < 10; ++i) {
        Tensor result = attention.forward(query, key, value);
    }
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    std::cout << "  10 attention forward passes: " << duration.count() << " ms" << std::endl;
    std::cout << "  Average per pass: " << duration.count() / 10.0 << " ms" << std::endl;
    
    // Benchmark conditioning
    printSubsectionHeader("Cross-Modal Conditioning Performance");
    start = std::chrono::high_resolution_clock::now();
    
    CrossModalConditioner conditioner;
    Tensor source = createMockModalityData("source", 8, 256);
    Tensor target = createMockModalityData("target", 8, 256);
    
    for (int i = 0; i < 20; ++i) {
        Tensor result = conditioner.conditionModality(target, source, "target", "source");
    }
    
    end = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    std::cout << "  20 conditioning operations: " << duration.count() << " ms" << std::endl;
    std::cout << "  Average per operation: " << duration.count() / 20.0 << " ms" << std::endl;
    
    // Benchmark bridge network
    printSubsectionHeader("Modal Bridge Network Performance");
    start = std::chrono::high_resolution_clock::now();
    
    ModalBridgeNetwork network({"visual", "audio", "text"});
    network.createBridge("visual", "audio");
    network.createBridge("audio", "text");
    
    Tensor features = createMockModalityData("features", 8, 128);
    for (int i = 0; i < 50; ++i) {
        Tensor bridged = network.bridgeFeatures(features, "visual", "audio");
    }
    
    end = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    std::cout << "  50 bridge operations: " << duration.count() << " ms" << std::endl;
    std::cout << "  Average per operation: " << duration.count() / 50.0 << " ms" << std::endl;
    
    std::cout << "âœ“ Performance benchmark completed" << std::endl;
}

int main() {
    std::cout << "CLModel Week 14: Cross-Modal Guidance Systems Demo" << std::endl;
    std::cout << "=================================================" << std::endl;
    std::cout << "Demonstrating advanced cross-modal conditioning, guidance mechanisms," << std::endl;
    std::cout << "dynamic adaptation, inter-modal communication, and real-time coordination." << std::endl;
    
    try {
        // Run all demonstrations
        demonstrateCrossModalAttention();
        demonstrateCrossModalConditioner();
        demonstrateGuidanceController();
        demonstrateModalBridgeNetwork();
        demonstrateMultiModalCoordinator();
        demonstrateCrossModalUtils();
        runPerformanceBenchmark();
        
        // Summary
        printSectionHeader("Week 14 Demo Summary");
        std::cout << "âœ“ Cross-modal attention mechanisms - PASSED" << std::endl;
        std::cout << "âœ“ Advanced guidance conditioning - PASSED" << std::endl;
        std::cout << "âœ“ Dynamic guidance adaptation - PASSED" << std::endl;
        std::cout << "âœ“ Inter-modal communication bridges - PASSED" << std::endl;
        std::cout << "âœ“ Multi-modal system coordination - PASSED" << std::endl;
        std::cout << "âœ“ Cross-modal utility functions - PASSED" << std::endl;
        std::cout << "âœ“ Performance benchmarking - PASSED" << std::endl;
        
        std::cout << "\nðŸŽ‰ Week 14: Cross-Modal Guidance Systems implementation complete!" << std::endl;
        std::cout << "All guidance mechanisms are operational and ready for integration." << std::endl;
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ Demo failed with exception: " << e.what() << std::endl;
        return 1;
    }
    
    return 0;
}
